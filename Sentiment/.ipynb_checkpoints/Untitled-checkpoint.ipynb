{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x8c in position 8: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-61fd4ee93ffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m     \u001b[1;31m#test()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-61fd4ee93ffb>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mword_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_2_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mcat_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_to_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcat_2_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\text-classification-demos-master\\util\\cnews_loader.py\u001b[0m in \u001b[0;36mword_2_id\u001b[1;34m(vocab_dir)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gb18030'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mword_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x8c in position 8: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "#coding=utf8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Created : 2018/12/26\n",
    "# Version : python2.7\n",
    "# Author  : yibo.li \n",
    "# File    : fasttext_model.py\n",
    "# Desc    : \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from util.cnews_loader import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "class FastText():\n",
    "    \"\"\"\n",
    "    文本分类，FastText模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_length, num_classes, vocab_size):\n",
    "        \"\"\"\n",
    "\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = 128\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name='input_y')\n",
    "        self.learning_rate = tf.placeholder(tf.float32, name='learn_rate')\n",
    "        self.inference()\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [self.vocab_size, self.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        # average vectors, to get representation of the sentence\n",
    "        with tf.name_scope(\"average\"):\n",
    "            mean_sentence = tf.reduce_mean(embedding_inputs, axis=1)\n",
    "\n",
    "        # linear classifier\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(mean_sentence, self.num_classes,\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n",
    "                                          name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1, name=\"pred\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.input_y)\n",
    "\n",
    "            l2_loss = tf.losses.get_regularization_loss()\n",
    "            self.loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "            self.loss += l2_loss\n",
    "\n",
    "            # optim\n",
    "            self.optim = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"acc\")\n",
    "\n",
    "\n",
    "def evaluate(sess, model, x_, y_):\n",
    "    \"\"\"\n",
    "    评估 val data 的准确率和损失\n",
    "    \"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 64)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = {model.input_x: x_batch, model.input_y: y_batch}\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def test_model(sess, graph, x_, y_):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sess:\n",
    "    :param graph:\n",
    "    :param x_:\n",
    "    :param y_:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 64)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    input_x = graph.get_operation_by_name('input_x').outputs[0]\n",
    "    input_y = graph.get_operation_by_name('input_y').outputs[0]\n",
    "    loss = graph.get_operation_by_name('loss/loss').outputs[0]\n",
    "    acc = graph.get_operation_by_name('accuracy/acc').outputs[0]\n",
    "\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = {input_x: x_batch, input_y: y_batch}\n",
    "        test_loss, test_acc = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "        total_loss += test_loss * batch_len\n",
    "        total_acc += test_acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "def main():\n",
    "    word_to_id, id_to_word = word_2_id(vocab_dir)\n",
    "    cat_to_id, id_to_cat = cat_2_id()\n",
    "\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, max_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, max_length)\n",
    "\n",
    "    epochs = 30\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    train_steps = 0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with tf.Graph().as_default():\n",
    "        seq_length = 512\n",
    "        num_classes = 10\n",
    "        vocab_size = 5000\n",
    "        fast_model = FastText(seq_length, num_classes, vocab_size)\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(epochs):\n",
    "                print('Epoch:', epoch + 1)\n",
    "                batch_train = batch_iter(x_train, y_train, 32)\n",
    "                for x_batch, y_batch in batch_train:\n",
    "                    train_steps += 1\n",
    "                    learn_rate = 0.001\n",
    "                    # learning rate vary\n",
    "                    feed_dict = {fast_model.input_x: x_batch, fast_model.input_y: y_batch,\n",
    "                                 fast_model.learning_rate: learn_rate}\n",
    "\n",
    "                    _, train_loss, train_acc = sess.run([fast_model.optim, fast_model.loss,\n",
    "                                                         fast_model.acc], feed_dict=feed_dict)\n",
    "\n",
    "                    if train_steps % 1000 == 0:\n",
    "                        val_loss, val_acc = evaluate(sess, fast_model, x_val, y_val)\n",
    "\n",
    "                    if val_acc > best_acc_val:\n",
    "                        # 保存最好结果\n",
    "                        best_acc_val = val_acc\n",
    "                        last_improved = train_steps\n",
    "                        saver.save(sess, \"./model/fast/model\", global_step=train_steps)\n",
    "                        # saver.save(sess=session, save_path=save_path)\n",
    "                        improved_str = '*'\n",
    "                    else:\n",
    "                        improved_str = ''\n",
    "\n",
    "                    now_time = datetime.now()\n",
    "                    msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                          + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                    print(msg.format(train_steps, train_loss, train_acc, val_loss, val_acc, now_time, improved_str))\n",
    "\n",
    "\n",
    "def test():\n",
    "    word_to_id, id_to_word = word_2_id(vocab_dir)\n",
    "    cat_to_id, id_to_cat = cat_2_id()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, max_length)\n",
    "    graph_path = \"./model/fast/model-40000.meta\"\n",
    "    model_path = \"./model/fast\"\n",
    "    graph = tf.Graph()\n",
    "    saver = tf.train.import_meta_graph(graph_path, graph=graph)\n",
    "    sess = tf.Session(graph=graph)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    test_loss, test_acc = test_model(sess, graph, x_test, y_test)\n",
    "    print(\"Test loss: %f, Test acc: %f\" %(test_loss, test_acc))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"./data/cnews\"\n",
    "    train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "    test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "    val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "    vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "    vocab_size = 5000\n",
    "    max_length = 512\n",
    "\n",
    "    if not os.path.exists(vocab_dir):\n",
    "        build_vocab(train_dir, vocab_dir, vocab_size)\n",
    "\n",
    "    main()\n",
    "    #test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
